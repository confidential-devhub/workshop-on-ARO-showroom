= Configure the OSC operator

Now that the OSC operator is installed, we need to set it up.

**Prerequisites**

To complete this section, it is mandatory to have all ARO credentials at hand defined in the xref:index.adoc#credentials[introduction], because they will have to be inserted in the various resources that we are going to create.

In order to create the xref:02-configure-osc.adoc#pp-cm[peer-pods configmap], it is necessary to have Trustee xref:01-install-trustee.adoc#twebui[installed] and xref:02-configure-trustee.adoc#trustee-route[configured].

[#feature-gate]
== Enable Confidential Containers feature gate

Create and apply `cc-fg.yaml` ConfigMap

[source,sh,role=execute]
----
cat > cc-fg.yaml <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: osc-feature-gates
  namespace: openshift-sandboxed-containers-operator
data:
  confidential: "true"
EOF

cat cc-fg.yaml
----

[source,sh,role=execute]
----
oc apply -f cc-fg.yaml
----

[#nodeport]
== Enable NodePort

Currently, only a Trustee route with a valid CA-signed certificate is supported. It is not possible to use a route with self-signed certificate. A non-production solution is to use `NodePort`, which is using unsecure http. This is not a supported approach for production and just useful for trying everything in a single cluster. For the purpose of this workshop, we will enable NodePort.

Get node IP and Kbs service nodeport, and then replace `TRUSTEE_URL` with the `http://ip:port` url:

[source,sh,role=execute]
----
nodePort=$(oc -n trustee-operator-system get service kbs-service -o=jsonpath={.spec.ports..nodePort})

nodeIP=$(oc get node -o wide | tail -1 | awk '/worker/{print $6}')

TRUSTEE_URL=http://${nodeIP}:${nodePort}

echo $TRUSTEE_HOST
----

[#pp-initdat]
== Create the initdata policy

The initdata specification provides a flexible way to initialize a CoCo peer pod with sensitive or workload-specific data at runtime, avoiding the need to embed such data in the virtual machine (VM) image. This enhances security by reducing exposure of confidential information and improves flexibility by eliminating custom image builds. For example, initdata can include three configuration settings:

* An X.509 certificate for secure communication.
* A cryptographic key for authentication.
* An optional Kata Agent policy.rego file to enforce runtime behavior when overriding the default Kata Agent policy.

We can apply an initdata configuration by using one of the following methods:

* Globally by including it in the peer pods config map, setting a cluster-wide default for all pods.
* On a specific pod when configuring a pod workload object, allowing customization for individual workloads.

In this section, we will create the initdata that will be set up as global in the configmap.
This same policy can also be modified and applied to a specific pod by using the `io.katacontainers.config.runtime.cc_init_data:` annotation under `metadata:annotations:` in the pod yaml spec.

In this policy, we will set the Trustee address in the internal CVM components. We will use `TRUSTEE_HOST` defined previously when xref:02-configure-trustee.adoc#trustee-route[configuring the Trustee].

[source,sh,role=execute]
----
cat > initdata.toml <<EOF
algorithm = "sha256"
version = "0.1.0"

[data]
"aa.toml" = '''
[token_configs]
[token_configs.coco_as]
url = '${TRUSTEE_HOST}'

[token_configs.kbs]
url = '${TRUSTEE_HOST}'
#cert = """
# <cert here>
#"""
'''

"cdh.toml"  = '''
socket = 'unix:///run/confidential-containers/cdh.sock'
credentials = []

[kbc]
name = 'cc_kbc'
url = '${TRUSTEE_HOST}'
#kbs_cert = """
# <cert here>
#"""
'''

#"policy.rego" = '''
# <policy here>
#'''
EOF

cat initdata.toml
----

Note that if `TRUSTEE_HOST` uses HTTPS with a CA-signed certificate (necessary for production environments), the certificate has to be added by uncommenting the block in `cert` under `[token_config.kbs]` and the one in `kbs_cert` under `[kbc]`. Otherwise, like in this demo, we can leave it commented out.

In addition, it is also possible to set a global kata agent policy (by default logs and exec are disabled). For more info on how to set up the agent policy, refer to the xref:03-deploy-workload.adoc#optional-options[optional deployment options]. In order to set the agent policy, uncomment the block `policy.rego` and insert the policy there. In this example, we will leave the default policy as it is.

Let's convert the policy in base64 and store it in the `INITDATA` variable.

[source,sh,role=execute]
----
INITDATA=$(base64 -w0 < initdata.toml)

echo $INITDATA
----

[#pp-cm]
== Create the peer-pods configmap

. Get the necessary credentials. In case you didn't do it at the beginning, run
`AZURE_RESOURCE_GROUP={azure_resource_group}`.
+
[source,sh,role=execute]
----
echo ""

# Get the ARO created RG
ARO_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}')

# Get the ARO region
ARO_REGION=$(oc get secret -n kube-system azure-credentials -o jsonpath="{.data.azure_region}" | base64 -d)

# Get VNET name used by ARO. This exists in the admin created RG.
# In this ARO infrastructure, there are 2 VNETs: pick the one starting with "aro-".
# The other is used internally by this workshop
ARO_VNET_NAME=$(az network vnet list --resource-group $AZURE_RESOURCE_GROUP --query "[].{Name:name} | [? contains(Name, 'aro')]" --output tsv)

# Get the Openshift worker subnet ip address cidr. This exists in the admin created RG
ARO_WORKER_SUBNET_ID=$(az network vnet subnet list --resource-group $AZURE_RESOURCE_GROUP --vnet-name $ARO_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv)

ARO_NSG_ID=$(az network nsg list --resource-group $ARO_RESOURCE_GROUP --query "[].{Id:id}" --output tsv)

echo "ARO_REGION: \"$ARO_REGION\""
echo "ARO_RESOURCE_GROUP: \"$ARO_RESOURCE_GROUP\""
echo "ARO_SUBNET_ID: \"$ARO_WORKER_SUBNET_ID\""
echo "ARO_NSG_ID: \"$ARO_NSG_ID\""
echo ""
----

. Create and apply the `peer-pods-configmap.yaml` ConfigMap. Note that at this point you must have already xref:02-configure-osc.adoc#pp-initdat[configured initdata] and got `${INITDATA}`.
+
[source,sh,role=execute]
----
cat > pp-cm.yaml <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_DC8es_v5"
  AZURE_INSTANCE_SIZES: "Standard_DC8as_v5,Standard_DC8ads_v5, Standard_DC8es_v5, Standard_DC8eds_v5"
  AZURE_RESOURCE_GROUP: "${ARO_RESOURCE_GROUP}"
  AZURE_REGION: "${ARO_REGION}"
  AZURE_SUBNET_ID: "${ARO_WORKER_SUBNET_ID}"
  AZURE_NSG_ID: "${ARO_NSG_ID}"
  AZURE_IMAGE_ID: ""
  PROXY_TIMEOUT: "5m"
  DISABLECVM: "false"
  ENABLE_SECURE_BOOT: "true"
  INITDATA: "${INITDATA}"
EOF

cat pp-cm.yaml
----
+
IMPORTANT: Note the `AZURE_INSTANCE_SIZES` field. This field is used to specify additional instance sizes that the operator should support. By default, if this field is not specified all new pod VM will be of type `AZURE_INSTANCE_SIZE`, but if it is, the operator will pick the smallest instance defined in `AZURE_INSTANCE_SIZES` that satisfies the memory and cpu limits defined in the podspec.
Azure instance types are explained and listed https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/overview?tabs=breakdownseries%2Cgeneralsizelist%2Ccomputesizelist%2Cmemorysizelist%2Cstoragesizelist%2Cgpusizelist%2Cfpgasizelist%2Chpcsizelist[here, window=blank].
Because of the quota limitations of this workshop, it is unlikely that it will be possible to deploy instances bigger than `Standard_DC8*`.

It is also worth pointing the azure terminology for instances: the pattern is usually `Standard_X{C}{num_cpus}{a or e}xx_vx` where `C` stands for **Confidential**, `a` for CPUs using **AMD SEV/SNP** technology, and `e` for CPUs using **Intel TDX** technology. Therefore `Standard_DC8_es_v5` is a confidential instance with 8 Intel CPUs using TDX to provide data in use confidentiality.

For this example, we are going to default the instance deployment AMD cpus, because they are available in all regions. If you want to deploy a TDX instance, check the catalog region availability (usually `northeurope` or `westeurope` are a good choice) and deploy a new workshop in that region.

[source,sh,role=execute]
----
oc apply -f pp-cm.yaml
----

NOTE: As you might have noticed, `AZURE_IMAGE_ID` is purposefully left empty. It will be filled in automatically by a Job created by the operator later.

[NOTE]
====
If you later update the peer pods config map, you must restart the osc-caa-ds daemonset to apply the changes.
After you update the config map, apply the manifest. Then restart the cloud-api-adaptor pods by running the following command:
[source,sh,role=execute]
----
oc set env ds/osc-caa-ds -n openshift-sandboxed-containers-operator REBOOT="$(date)"
----
Keep in mind that restarting the daemonset recreates the peer pods, it does not update the existing pods
====

[#pp-key]
== Create the peer-pods SSH key
When CoCo mode is disabled (so the VM is not confidential), this key is also useful to enter the pod VM, inspect it and debug. In CoCo, ssh into the VM is disabled by default. We need to create it anyways because an SSH key is required to create Azure VMs, but as we will see, it will be discarded immediately.

. Create an ssh key:
+
[source,sh,role=execute]
----
ssh-keygen -f ./id_rsa -N ""
----

. Upload `id_rsa.pub` as `Secret` into Openshift.
+
[source,sh,role=execute]
----
oc create secret generic ssh-key-secret -n openshift-sandboxed-containers-operator --from-file=id_rsa.pub=./id_rsa.pub
----
. Once the public key is uploaded, delete both private and public from the local setup.
+
[source,sh,role=execute]
----
shred --remove id_rsa.pub id_rsa
----

[#pp-kc]
== Create the peer-pods KataConfig

You must create a `KataConfig` custom resource (CR) to install `kata-remote` as a runtime class on your worker nodes. This is a core operation that enables the worker nodes to create VMs.

Creating the `KataConfig` CR triggers the Openshift sandboxed containers Operator to create a `RuntimeClass` CR named `kata-remote` with a default configuration. This enables users to configure workloads to use `kata-remote` as the runtime by referencing the CR in the `RuntimeClassName` field. This CR also specifies the resource overhead for the runtime.

Openshift sandboxed containers installs `kata-remote` as a _secondary, optional_ runtime on the cluster and not as the primary runtime.

[IMPORTANT]
====
Creating the KataConfig CR automatically reboots the worker nodes. According with the documentation, the reboot can take from 10 to more than 60 minutes. **In this ARO workshop, it should take around 15 minutes**. Factors that impede reboot time are as follows:

* A larger Openshift Container Platform deployment with a greater number of worker nodes.
* Activation of the BIOS and Diagnostics utility.
* Deployment on a hard disk drive rather than an SSD.
* Deployment on physical nodes such as bare metal, rather than on virtual nodes.
* A slow CPU and network.
====

. Create a KataConfig CDR and apply it. By default all worker nodes will be configured to run CoCo workloads. If you want to restrict it to specific worker nodes, then add any specific label to those worker does and update the `kataconfigPoolSelector`. For this workshop, it is not needed to add any label.
+
[source,sh,role=execute]
----
cat > kataconfig.yaml <<EOF
apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
 name: example-kataconfig
spec:
  enablePeerPods: true
#  kataConfigPoolSelector:
#    matchLabels:
#      <label_key>: '<label_value>'  # Fill with your node labels
EOF

cat kataconfig.yaml
----
+
[source,sh,role=execute]
----
oc apply -f kataconfig.yaml
----

. Wait for kata-oc `MachineConfigPool` (MCP) to be in `UPDATED` state (once `UPDATEDMACHINECOUNT` equals `MACHINECOUNT`). In this ARO setup with 3 worker nodes, it should take around 15 minutes.
+
[source,sh,role=execute]
----
watch oc get mcp/kata-oc
----
+
Expected output after all nodes have been updated:
+
[source,texinfo,subs="attributes"]
----
NAME      CONFIG                                              UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
kata-oc   rendered-kata-oc-894630a1c9cdf3ebef8bd98c72e26608   True      False      False      3              3                   3                     0                      13m
----

=== Verification
. Make sure that the `AZURE_IMAGE_ID` in the `ConfigMap` is populated. If it isn't, make sure there is a job running called `osc-podvm-image-creation-<random-letters>`.
+
[source,sh,role=execute]
----
oc get configmap peer-pods-cm -n openshift-sandboxed-containers-operator -o yaml
----
+
If `data: AZURE_IMAGE_ID:` is still empty, check the job:
+
[source,sh,role=execute]
----
watch oc get pods -n openshift-sandboxed-containers-operator
----
+
Wait till the job `STATUS` doesn't change to `Completed`. In this ARO setup, it should take around 15 minutes.


. Make sure that the required daemonset is created.
+
[source,sh,role=execute]
----
oc get -n openshift-sandboxed-containers-operator ds/osc-caa-ds
----
+
Expected output:
+
[source,texinfo,subs="attributes"]
----
NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                      AGE
osc-caa-ds   3         3         3       3            3           node-role.kubernetes.io/kata-oc=   22m
----

. Make sure the `RuntimeClass` are created.
+
[source,sh,role=execute]
----
oc get runtimeclass
----
+
Expected output:
+
[source,texinfo,subs="attributes"]
----
NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m
----

This is the expected output when looking at the OSC pods (note the random character ending will change):
[source,sh,role=execute]
----
oc get pods -n openshift-sandboxed-containers-operator
----

[source,texinfo,subs="attributes"]
----
NAME                                           READY   STATUS      RESTARTS   AGE
controller-manager-5dd87698b7-9cqbn            2/2     Running     0          17m
openshift-sandboxed-containers-monitor-m9ffw   1/1     Running     0          30m
openshift-sandboxed-containers-monitor-sdlz4   1/1     Running     0          30m
openshift-sandboxed-containers-monitor-z8zh5   1/1     Running     0          30m
osc-podvm-image-creation-fltm8                 0/1     Completed   0          17m
peer-pods-webhook-65cffdd499-2nh9q             1/1     Running     0          2m59s
peer-pods-webhook-65cffdd499-8x684             1/1     Running     0          2m59s
osc-caa-ds-hl7fb            1/1     Running     0          2m59s
osc-caa-ds-s6xkk            1/1     Running     0          2m59s
osc-caa-ds-vkfm5            1/1     Running     0          2m59s
----

This is it! Now the cluster is ready to run workloads with `kata-remote` `RuntimeClass`!